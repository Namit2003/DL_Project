{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "Xza33OrY9I7I",
    "outputId": "17f541ca-f801-4600-85d4-a057532c413f"
   },
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1, fixed_image_standardization, training\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "from PIL import Image\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'Faces'\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "workers = 0 if os.name == 'nt' else 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(\n",
    "    image_size=160, margin=0, min_face_size=30,\n",
    "    thresholds=[0.4, 0.4, 0.4], factor=0.709, post_process=True,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(data_dir, transform=transforms.Resize((512, 512)))\n",
    "dataset.samples = [\n",
    "    (p, p.replace(data_dir, data_dir + '_cropped'))\n",
    "        for p, _ in dataset.samples\n",
    "]\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    num_workers=workers,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=training.collate_pil\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10 of 10"
     ]
    }
   ],
   "source": [
    "for i, (x, y) in enumerate(loader):\n",
    "    # print(len(x))\n",
    "    for img,path in zip(x,y):\n",
    "        mtcnn(img,save_path=path)\n",
    "    print('\\rBatch {} of {}'.format(i + 1, len(loader)), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove mtcnn to reduce GPU memory usage\n",
    "del mtcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "        transforms.RandomResizedCrop(size=(128, 128), scale=(0.8, 1.0)),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        transforms.GaussianBlur(kernel_size=3),\n",
    "        transforms.Resize(size=(160, 160)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform = transforms.Compose([transform])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolder(root='Faces_cropped', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images saved successfully.\n"
     ]
    }
   ],
   "source": [
    "save_dir = 'augmented/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save augmented images\n",
    "for i, (images, _) in enumerate(dataloader):\n",
    "    for j, image in enumerate(images):\n",
    "        img_path = os.path.join(save_dir, f'image_{i * len(images) + j}.jpg')\n",
    "        torchvision.utils.save_image(image, img_path)\n",
    "\n",
    "print(\"Images saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = InceptionResnetV1(\n",
    "    classify=True,\n",
    "    pretrained='vggface2',\n",
    "    num_classes=len(dataset.class_to_idx)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(resnet.parameters(), lr=0.001)\n",
    "scheduler = MultiStepLR(optimizer, [5, 10])\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    np.float32,\n",
    "    transforms.ToTensor(),\n",
    "    fixed_image_standardization\n",
    "])\n",
    "dataset = datasets.ImageFolder(data_dir + '_cropped', transform=trans)\n",
    "img_inds = np.arange(len(dataset))\n",
    "np.random.shuffle(img_inds)\n",
    "train_inds = img_inds[:int(0.8 * len(img_inds))]\n",
    "val_inds = img_inds[int(0.8 * len(img_inds)):]\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    num_workers=workers,\n",
    "    batch_size=batch_size,\n",
    "    sampler=SubsetRandomSampler(train_inds)\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset,\n",
    "    num_workers=workers,\n",
    "    batch_size=batch_size,\n",
    "    sampler=SubsetRandomSampler(val_inds)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "metrics = {\n",
    "    'fps': training.BatchTimer(),\n",
    "    'acc': training.accuracy\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Initial\n",
      "----------\n",
      "Valid |     1/1    | loss:    0.6948 | fps:   62.0697 | acc:    0.5185   \n",
      "\n",
      "Epoch 1/20\n",
      "----------\n",
      "Train |     4/4    | loss:    0.5365 | fps:   37.2436 | acc:    0.7266   \n",
      "Valid |     1/1    | loss:   67.0453 | fps:  157.1057 | acc:    0.2963   \n",
      "\n",
      "Epoch 2/20\n",
      "----------\n",
      "Train |     4/4    | loss:    0.3829 | fps:   46.2814 | acc:    0.9219   \n",
      "Valid |     1/1    | loss: 2986.6252 | fps:  171.0252 | acc:    0.2963   \n",
      "\n",
      "Epoch 3/20\n",
      "----------\n",
      "Train |     4/4    | loss:    0.2077 | fps:   47.9488 | acc:    0.9516   \n",
      "Valid |     1/1    | loss: 1836.7736 | fps:  155.5471 | acc:    0.2963   \n",
      "\n",
      "Epoch 4/20\n",
      "----------\n",
      "Train |     4/4    | loss:    0.0161 | fps:   48.3089 | acc:    1.0000   \n",
      "Valid |     1/1    | loss:  602.8787 | fps:  172.7942 | acc:    0.2963   \n",
      "\n",
      "Epoch 5/20\n",
      "----------\n",
      "Train |     4/4    | loss:    0.6076 | fps:   49.3834 | acc:    0.8687   \n",
      "Valid |     1/1    | loss:  122.6568 | fps:  172.8129 | acc:    0.4074   \n",
      "\n",
      "Epoch 6/20\n",
      "----------\n",
      "Train |     4/4    | loss:    0.0624 | fps:   48.3306 | acc:    0.9844   \n",
      "Valid |     1/1    | loss:    2.3887 | fps:  169.4521 | acc:    0.6667   \n",
      "\n",
      "Epoch 7/20\n",
      "----------\n",
      "Train |     4/4    | loss:    0.0568 | fps:   48.3841 | acc:    0.9844   \n",
      "Valid |     1/1    | loss:    0.3887 | fps:  155.2424 | acc:    0.8519   \n",
      "\n",
      "Epoch 8/20\n",
      "----------\n",
      "Train |     4/4    | loss:    0.0783 | fps:   49.0158 | acc:    0.9844   \n",
      "Valid |     1/1    | loss:    0.3832 | fps:  156.3658 | acc:    0.8519   \n",
      "\n",
      "Epoch 9/20\n",
      "----------\n",
      "Train |     4/4    | loss:    0.0326 | fps:   48.3179 | acc:    0.9922   \n",
      "Valid |     1/1    | loss:    0.3654 | fps:  155.1095 | acc:    0.8519   \n",
      "\n",
      "Epoch 10/20\n",
      "----------\n",
      "Train |     4/4    | loss:    0.0284 | fps:   49.3462 | acc:    0.9922   \n",
      "Valid |     1/1    | loss:    0.3436 | fps:  158.5522 | acc:    0.8519   \n",
      "\n",
      "Epoch 11/20\n",
      "----------\n",
      "Train |     4/4    | loss:    0.0156 | fps:   49.7989 | acc:    1.0000   \n",
      "Valid |     1/1    | loss:    0.3282 | fps:  156.3811 | acc:    0.8519   \n",
      "\n",
      "Epoch 12/20\n",
      "----------\n",
      "Train |     4/4    | loss:    0.0223 | fps:   49.0473 | acc:    1.0000   \n",
      "Valid |     1/1    | loss:    0.3182 | fps:  172.7974 | acc:    0.8519   \n",
      "\n",
      "Epoch 13/20\n",
      "----------\n",
      "Train |     4/4    | loss:    0.0296 | fps:   49.5997 | acc:    0.9922   \n",
      "Valid |     1/1    | loss:    0.3098 | fps:  157.0916 | acc:    0.8519   \n",
      "\n",
      "Epoch 14/20\n",
      "----------\n",
      "Train |     4/4    | loss:    0.0164 | fps:   48.8035 | acc:    1.0000   \n",
      "Valid |     1/1    | loss:    0.3112 | fps:  155.4570 | acc:    0.8519   \n",
      "\n",
      "Epoch 15/20\n",
      "----------\n",
      "Train |     4/4    | loss:    0.0303 | fps:   47.9140 | acc:    1.0000   \n",
      "Valid |     1/1    | loss:    0.3024 | fps:  163.0826 | acc:    0.8889   \n",
      "\n",
      "Epoch 16/20\n",
      "----------\n",
      "Train |     4/4    | loss:    0.0321 | fps:   49.1773 | acc:    1.0000   \n",
      "Valid |     1/1    | loss:    0.3021 | fps:  159.1444 | acc:    0.8889   \n",
      "\n",
      "Epoch 17/20\n",
      "----------\n",
      "Train |     4/4    | loss:    0.0332 | fps:   48.6885 | acc:    0.9750   \n",
      "Valid |     1/1    | loss:    0.3100 | fps:  172.8393 | acc:    0.8889   \n",
      "\n",
      "Epoch 18/20\n",
      "----------\n",
      "Train |     4/4    | loss:    0.0244 | fps:   46.7543 | acc:    1.0000   \n",
      "Valid |     1/1    | loss:    0.3240 | fps:  132.8794 | acc:    0.8519   \n",
      "\n",
      "Epoch 19/20\n",
      "----------\n",
      "Train |     4/4    | loss:    0.0226 | fps:   47.6615 | acc:    1.0000   \n",
      "Valid |     1/1    | loss:    0.3137 | fps:  157.1164 | acc:    0.8519   \n",
      "\n",
      "Epoch 20/20\n",
      "----------\n",
      "Train |     4/4    | loss:    0.0258 | fps:   48.0176 | acc:    0.9922   \n",
      "Valid |     1/1    | loss:    0.3045 | fps:  157.0892 | acc:    0.8889   \n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "writer.iteration, writer.interval = 0, 10\n",
    "\n",
    "print('\\n\\nInitial')\n",
    "print('-' * 10)\n",
    "resnet.eval()\n",
    "training.pass_epoch(\n",
    "    resnet, loss_fn, val_loader,\n",
    "    batch_metrics=metrics, show_running=True, device=device,\n",
    "    writer=writer\n",
    ")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('\\nEpoch {}/{}'.format(epoch + 1, epochs))\n",
    "    print('-' * 10)\n",
    "\n",
    "    resnet.train()\n",
    "    training.pass_epoch(\n",
    "        resnet, loss_fn, train_loader, optimizer, scheduler,\n",
    "        batch_metrics=metrics, show_running=True, device=device,\n",
    "        writer=writer\n",
    "    )\n",
    "\n",
    "    resnet.eval()\n",
    "    training.pass_epoch(\n",
    "        resnet, loss_fn, val_loader,\n",
    "        batch_metrics=metrics, show_running=True, device=device,\n",
    "        writer=writer\n",
    "    )\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6aNOYxGm9qh4"
   },
   "outputs": [],
   "source": [
    "mtcnn0=MTCNN(image_size=240,margin=0,keep_all=False,min_face_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LgFzELMOFS1m"
   },
   "outputs": [],
   "source": [
    "mtcnn=MTCNN(image_size=240,margin=0,keep_all=True,min_face_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "2MiUrUMSFeMS"
   },
   "outputs": [],
   "source": [
    "dataset=datasets.ImageFolder('Faces')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "m2oKJPkRGQug"
   },
   "outputs": [],
   "source": [
    "idx_to_class={i:c for c,i in dataset.class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "LA2ZsTe7JNH3"
   },
   "outputs": [],
   "source": [
    "def collate_fn(x):\n",
    "  return x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "omyyTRS8Hn6n"
   },
   "outputs": [],
   "source": [
    "loader=DataLoader(dataset,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "1vRJuzfzIapN"
   },
   "outputs": [],
   "source": [
    "name_list=[]\n",
    "embedding_list=[]\n",
    "\n",
    "resnet.to(device)\n",
    "\n",
    "for img,idx in loader:\n",
    "  # print(img)\n",
    "  # print(idx)\n",
    "  face,prob=mtcnn0(img,return_prob=True)\n",
    "  if face is not None and prob>0.9:\n",
    "    face = face.to(device)\n",
    "    emb=resnet((face.unsqueeze(0)))\n",
    "    embedding_list.append(emb)\n",
    "    name_list.append(idx_to_class[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving embedding_list and name_list\n",
    "with open('NamitAryan20_emb/embedding_list.pkl', 'wb') as f:\n",
    "    pickle.dump(embedding_list, f)\n",
    "\n",
    "with open('NamitAryan20_emb/name_list.pkl', 'wb') as f:\n",
    "    pickle.dump(name_list, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading embedding_list and name_list\n",
    "with open('NamitAryan20_emb/embedding_list.pkl', 'rb') as f:\n",
    "    embedding_list = pickle.load(f)\n",
    "\n",
    "with open('NamitAryan20_emb/name_list.pkl', 'rb') as f:\n",
    "    name_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "jeb6RC1VJT--"
   },
   "outputs": [],
   "source": [
    "# cv2.namedWindow(\"preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video = cv2.VideoCapture('20240402_182307.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fps = math.ceil(video.get(cv2.CAP_PROP_FPS))\n",
    "# print('frames per second =',fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not video.isOpened():\n",
    "#     print(\"Error: Could not open video.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interval_frames = int(fps * 1/30)\n",
    "# print(interval_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames per second = 31\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_7288\\1155871887.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  face_tensor = torch.tensor(img_cropped_list[i], dtype=torch.float32).unsqueeze(0).to(device).detach()\n"
     ]
    }
   ],
   "source": [
    "cv2.namedWindow(\"preview\")\n",
    "# video = cv2.VideoCapture('20240402_182307.mp4')\n",
    "video = cv2.VideoCapture('20240418_122036.mp4')\n",
    "# video = cv2.VideoCapture('20240418_124735.mp4')\n",
    "fps = math.ceil(video.get(cv2.CAP_PROP_FPS))\n",
    "print('frames per second =',fps)\n",
    "\n",
    "if not video.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "\n",
    "interval_frames = int(fps * 1/30)\n",
    "print(interval_frames)\n",
    "\n",
    "target_size = (1536, 864)\n",
    "\n",
    "fc=0\n",
    "resnet.to(device)\n",
    "while True:\n",
    "  ret,frame=video.read()\n",
    "#   print(frame.shape)\n",
    "#   break\n",
    "  if not ret:\n",
    "    break\n",
    "  \n",
    "  resized_frame = cv2.resize(frame, target_size)\n",
    "  \n",
    "  if fc%interval_frames==0:\n",
    "      img=Image.fromarray(resized_frame)\n",
    "      img_cropped_list,prob_list=mtcnn(img,return_prob=True)\n",
    "      if img_cropped_list is not None:\n",
    "         boxes,_=mtcnn.detect(img)\n",
    "         for i,prob in enumerate(prob_list):\n",
    "             if prob>0.9:\n",
    "                 face_tensor = torch.tensor(img_cropped_list[i], dtype=torch.float32).unsqueeze(0).to(device).detach()\n",
    "                 emb = resnet(face_tensor).detach()\n",
    "                #  emb=resnet(img_cropped_list[i].unsqueeze(0)).detach()\n",
    "                 dist_list=[]\n",
    "                 for idx,emb_db in enumerate(embedding_list):\n",
    "                     dist=torch.dist(emb,emb_db).item()\n",
    "                     dist_list.append(dist)\n",
    "                 min_dist=min(dist_list)\n",
    "                 box=boxes[i]\n",
    "                 if min_dist<0.2:\n",
    "                     min_dist_idx=dist_list.index(min_dist)\n",
    "                     name=name_list[min_dist_idx]\n",
    "                #  original_frame=resized_frame.copy()\n",
    "                     resized_frame = cv2.putText(resized_frame, name+' '+str(min_dist), (int(box[0]),int(box[1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0),1, cv2.LINE_AA)\n",
    "                 resized_frame = cv2.rectangle(resized_frame, (int(box[0]),int(box[1])) , (int(box[2]),int(box[3])), (255,0,0), 2)\n",
    "      cv2.imshow(\"preview\",resized_frame)\n",
    "      if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "          break\n",
    "  fc+= 1\n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_17148\\606889315.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  face_tensor = torch.tensor(img_cropped_list[i], dtype=torch.float32).unsqueeze(0).to(device)\n"
     ]
    }
   ],
   "source": [
    "# cv2.namedWindow(\"preview\")\n",
    "# cam=cv2.VideoCapture(4)\n",
    "\n",
    "# target_size = (1000, 500)\n",
    "\n",
    "# fc=0\n",
    "# resnet.to(device)\n",
    "# while True:\n",
    "#   ret,frame=cam.read()\n",
    "# #   print(frame.shape)\n",
    "# #   break\n",
    "#   if not ret:\n",
    "#     break\n",
    "  \n",
    "#   resized_frame = cv2.resize(frame, target_size)\n",
    "  \n",
    "#   if fc%interval_frames==0:\n",
    "#       img=Image.fromarray(resized_frame)\n",
    "#       img_cropped_list,prob_list=mtcnn(img,return_prob=True)\n",
    "#       if img_cropped_list is not None:\n",
    "#          boxes,_=mtcnn.detect(img)\n",
    "#          for i,prob in enumerate(prob_list):\n",
    "#              if prob>0.9:\n",
    "#                  face_tensor = torch.tensor(img_cropped_list[i], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "#                  emb = resnet(face_tensor).detach()\n",
    "#                 #  emb=resnet(img_cropped_list[i].unsqueeze(0)).detach()\n",
    "#                  dist_list=[]\n",
    "#                  for idx,emb_db in enumerate(embedding_list):\n",
    "#                      dist=torch.dist(emb,emb_db).item()\n",
    "#                      dist_list.append(dist)\n",
    "#                  min_dist=min(dist_list)\n",
    "#                  min_dist_idx=dist_list.index(min_dist)\n",
    "#                  name=name_list[min_dist_idx]\n",
    "#                  box=boxes[i]\n",
    "#                  original_frame=resized_frame.copy()\n",
    "#                  if min_dist<0.9:\n",
    "#                      resized_frame = cv2.putText(resized_frame, name+' '+str(min_dist), (int(box[0]),int(box[1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0),1, cv2.LINE_AA)\n",
    "#                  resized_frame = cv2.rectangle(resized_frame, (int(box[0]),int(box[1])) , (int(box[2]),int(box[3])), (255,0,0), 2)\n",
    "#       cv2.imshow(\"preview\",resized_frame)\n",
    "#       if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "#           break\n",
    "#   fc+= 1\n",
    "# video.release()\n",
    "# cv2.destroyAllWindows()\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
